{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d14c0ce",
   "metadata": {},
   "source": [
    "# 优化模型参数\n",
    "\n",
    "- 译文：https://pytorch.apachecn.org/2.0/tutorials/beginner/basics/optimization_tutorial\n",
    "- 原文：https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b1dc4e",
   "metadata": {},
   "source": [
    "## 概览\n",
    "\n",
    "- 训练模型是一个迭代过程：每个 epoch 包含一次训练循环与一次验证/测试循环。\n",
    "- 每次训练迭代包括：前向传播、计算损失、反向传播（梯度计算）、根据梯度更新参数。\n",
    "- 本文展示完整的训练/测试循环实现、超参数设置与示例输出说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a3e8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f0abb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "# 模型定义（与之前章节一致）\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652618fa",
   "metadata": {},
   "source": [
    "## 超参数\n",
    "\n",
    "常见超参数示例：迭代次数 (epochs)、批大小 (batch_size)、学习率 (learning_rate)。\n",
    "\n",
    "下面设置示例超参数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5f3e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "# 注意：DataLoader 的 batch_size 与此处变量应保持一致（示例中已在前面设置为64）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f211f8ec",
   "metadata": {},
   "source": [
    "## 损失函数与优化器\n",
    "\n",
    "- 损失函数衡量模型输出与真实标签之间的差距。分类任务常用 `nn.CrossEntropyLoss()`。\n",
    "- 优化器根据梯度更新模型参数，常见优化器有 SGD、Adam 等。下面用 SGD 示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860de282",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f1b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss_val, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss_val:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cd4119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 运行训练与测试循环（示例设置 epochs）\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5605d36",
   "metadata": {},
   "source": [
    "## 小结与建议\n",
    "\n",
    "- 训练步骤：清零梯度 -> 前向传播 -> 计算损失 -> 反向传播 -> 优化器更新。\n",
    "- 注意在训练前调用 `model.train()`，在测试/验证时调用 `model.eval()` 并使用 `torch.no_grad()` 来禁用梯度计算。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
