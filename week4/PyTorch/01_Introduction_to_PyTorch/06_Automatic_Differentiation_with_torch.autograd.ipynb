{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50b9fb24",
   "metadata": {},
   "source": [
    "# 自动微分运算 — torch.autograd\n",
    "- 译文：https://pytorch.apachecn.org/2.0/tutorials/beginner/basics/autogradqs_tutorial\n",
    "- 原文：https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ed791f",
   "metadata": {},
   "source": [
    "## 概览\n",
    "\n",
    "- 在反向传播算法中，模型参数根据损失函数对参数的梯度进行更新。\n",
    "- `torch.autograd` 是 PyTorch 的自动微分引擎，会为张量构建动态的计算图并自动计算梯度。\n",
    "- 本笔记演示如何：设置 `requires_grad`、观察 `grad_fn`、调用 `backward()`、禁用梯度追踪，以及计算雅可比乘积（Jacobian-vector product）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43d4a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# 简单单层网络示例，展示计算图与 grad_fn\n",
    "x = torch.ones(5)                 # 输入 tensor\n",
    "y = torch.zeros(3)                # 期望输出\n",
    "w = torch.randn(5, 3, requires_grad=True)  # 需求梯度的参数\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "\n",
    "z = torch.matmul(x, w) + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "\n",
    "print(f\"Gradient function for z = {z.grad_fn}\")\n",
    "print(f\"Gradient function for loss = {loss.grad_fn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfebd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算梯度：对 loss 调用 backward，然后读取参数的 grad 属性\n",
    "loss.backward()\n",
    "print('w.grad:')\n",
    "print(w.grad)\n",
    "print('\\nb.grad:')\n",
    "print(b.grad)\n",
    "\n",
    "# 注意：在实际训练循环中，通常会在每次迭代前把梯度清零：optimizer.zero_grad() 或 param.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f79bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 禁用梯度追踪的两种方法：torch.no_grad() 和 detach()\n",
    "z = torch.matmul(x, w) + b\n",
    "print('z.requires_grad before no_grad:', z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z2 = torch.matmul(x, w) + b\n",
    "    print('z2.requires_grad inside no_grad:', z2.requires_grad)\n",
    "\n",
    "z_det = z.detach()\n",
    "print('z_det.requires_grad after detach():', z_det.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea1c06c",
   "metadata": {},
   "source": [
    "## 计算图（DAG）与反向传播原理\n",
    "\n",
    "- `autograd` 在前向传播时构建由 `Function` 对象组成的有向无环图（DAG），跟踪每次运算并保存反向传播所需的函数引用（保存在 `tensor.grad_fn`）。\n",
    "- 调用 `loss.backward()` 会从根节点（loss）开始按链式法则向后计算梯度，并把结果累加到叶子节点（设置了 `requires_grad=True` 的张量）的 `.grad` 属性中。\n",
    "- 注意：计算图是动态重建的；每次前向传播都会创建新的图。若对同一图多次调用 `backward()`，需使用 `retain_graph=True` 或在每次调用前清理/重建图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa97567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 雅可比（Jacobian）乘积示例：对非标量输出调用 backward 时需传入向量参数\n",
    "inp = torch.eye(4, 5, requires_grad=True)\n",
    "out = (inp + 1).pow(2).t()\n",
    "# 传入与 out 相同形状的张量作为 backward 的参数\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print('First call\\n', inp.grad)\n",
    "# 第二次调用会累加梯度\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print('\\nSecond call\\n', inp.grad)\n",
    "# 清零再调用\n",
    "inp.grad.zero_()\n",
    "out.backward(torch.ones_like(out), retain_graph=True)\n",
    "print('\\nCall after zeroing gradients\\n', inp.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b14c2f",
   "metadata": {},
   "source": [
    "## 小结与参考\n",
    "\n",
    "- 将模型参数设为 `requires_grad=True`，前向构建计算图，调用 `backward()` 自动计算梯度，并从参数的 `.grad` 中读取结果。\n",
    "- 在评估或推理阶段使用 `torch.no_grad()` 或 `tensor.detach()` 禁用梯度追踪以加速计算并节省内存。\n",
    "- 对非标量输出可以通过向 `backward()` 传入合适形状的向量来计算雅可比乘积。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
